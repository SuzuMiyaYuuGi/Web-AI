import json
from ollama_utils import ask_llama
from pythainlp.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer

# ----------------------------- NEW: product search deps -----------------------------
import os
import re
import pandas as pd
from typing import List, Tuple

PRODUCT_CSV_PATH = "Product List.csv"
MAX_PRODUCTS_TO_SHOW = 5
# ------------------------------------------------------------------------------------

# Load data from JSON (‡πÄ‡∏î‡∏¥‡∏°)
with open("output_data.json", encoding="utf-8") as f:
    docs = json.load(f)

# Improved Token Matching
def tokenize_and_clean(text: str):
    return word_tokenize(text.lower(), engine='newmm')

# Use TF-IDF for more context-aware document retrieval (‡πÄ‡∏î‡∏¥‡∏°)
def find_best_context(question: str):
    question_words = tokenize_and_clean(question)
    corpus = [entry.get("HTML", "") + " " + entry.get("Header", "") + " " + entry.get("Tag", "") for entry in docs]
    vectorizer = TfidfVectorizer(stop_words="english")
    tfidf_matrix = vectorizer.fit_transform(corpus)
    question_vec = vectorizer.transform([" ".join(question_words)])
    similarities = tfidf_matrix * question_vec.T
    scores = similarities.toarray().flatten()
    best_match_idx = scores.argmax()
    if scores[best_match_idx] == 0:
        return None
    return docs[best_match_idx]

# Combine metadata into the context to enhance response generation (‡πÄ‡∏î‡∏¥‡∏°)
def build_prompt(question: str, context_html: str, metadata: dict):
    base = """
‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏≤‡∏ï‡∏≠‡∏ö‡πÄ‡∏•‡∏¢‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö ‡∏°‡∏µ/‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏´‡∏ô ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤/‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ ‡πÑ‡∏´‡∏°‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤/‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏±‡πâ‡∏ô‡πÜ
‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏î‡πÜ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö

‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î:
- ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
- ‡∏´‡∏≤‡∏Å‡∏™‡∏£‡∏∏‡∏õ ‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏™‡∏±‡πâ‡∏ô‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
"""
    meta = f"""[‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö]: {metadata.get("NamePage", "")}
[‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà/‡∏®‡∏π‡∏ô‡∏¢‡πå]: {metadata.get("Center", "")}
[‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠]: {metadata.get("Header", "")}
[‡πÅ‡∏ó‡πá‡∏Å]: {metadata.get("Tag", "")}"""

    return f"""{base}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:
{question}

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå:
{meta}

[HTML ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå]:
{context_html}

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
"""

# ================================== NEW: PRODUCT SEARCH ==================================

def _normalize_product_columns(df: pd.DataFrame) -> pd.DataFrame:
    """‡∏ó‡∏≥‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô: ID / ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ / ‡∏®‡∏π‡∏ô‡∏¢‡πå / link"""
    rename_map = {}
    lower_map = {c.lower().strip(): c for c in df.columns}

    if "id" in lower_map: rename_map[lower_map["id"]] = "ID"
    for key in ["name", "product", "product name", "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤"]:
        if key in lower_map: rename_map[lower_map[key]] = "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤"; break
    for key in ["center", "‡∏®‡∏π‡∏ô‡∏¢‡πå", "‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏á‡∏≤‡∏ô", "‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"]:
        if key in lower_map: rename_map[lower_map[key]] = "‡∏®‡∏π‡∏ô‡∏¢‡πå"; break
    for key in ["link", "url", "‡∏•‡∏¥‡∏á‡∏Å‡πå", "‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå"]:
        if key in lower_map: rename_map[lower_map[key]] = "link"; break

    df = df.rename(columns=rename_map)
    for col in ["ID", "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏®‡∏π‡∏ô‡∏¢‡πå", "link"]:
        if col not in df.columns:
            df[col] = ""
    return df[["ID", "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏®‡∏π‡∏ô‡∏¢‡πå", "link"]].copy()

def _load_products() -> pd.DataFrame:
    if not os.path.exists(PRODUCT_CSV_PATH):
        return pd.DataFrame(columns=["ID", "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏®‡∏π‡∏ô‡∏¢‡πå", "link"])
    # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ encoding
    encodings = [None, "utf-8-sig", "cp874", "latin-1"]
    last_err = None
    df = None
    for enc in encodings:
        try:
            df = pd.read_csv(PRODUCT_CSV_PATH, encoding=enc) if enc else pd.read_csv(PRODUCT_CSV_PATH)
            break
        except Exception as e:
            last_err = e
    if df is None:
        return pd.DataFrame(columns=["ID", "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏®‡∏π‡∏ô‡∏¢‡πå", "link"])

    df = _normalize_product_columns(df)
    # ‡∏ó‡∏≥‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô str ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô NaN
    for c in ["ID", "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏®‡∏π‡∏ô‡∏¢‡πå", "link"]:
        df[c] = df[c].astype(str).fillna("").str.strip()
    return df

_PRODUCTS_DF = _load_products()

def _extract_possible_ids(text: str) -> List[str]:
    """‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ (‡∏≠‡∏±‡∏Å‡∏©‡∏£/‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏Ç‡∏µ‡∏î‡∏•‡πà‡∏≤‡∏á/‡∏Ç‡∏µ‡∏î‡∏Å‡∏•‡∏≤‡∏á) ‡∏¢‡∏≤‡∏ß >= 3"""
    tokens = re.findall(r"[A-Za-z0-9_-]{3,}", text)
    return [t for t in tokens if not re.search(r"[‡∏Å-‡πô]", t)]

def _tfidf_rank_products(query: str, df: pd.DataFrame, topk: int = 10) -> List[Tuple[int, float]]:
    """‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢ TF-IDF ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ID/‡∏ä‡∏∑‡πà‡∏≠/‡∏®‡∏π‡∏ô‡∏¢‡πå/‡∏•‡∏¥‡∏á‡∏Å‡πå"""
    if df.empty:
        return []
    corpus = (df["ID"].fillna("") + " " +
              df["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤"].fillna("") + " " +
              df["‡∏®‡∏π‡∏ô‡∏¢‡πå"].fillna("") + " " +
              df["link"].fillna("")).tolist()
    vec = TfidfVectorizer()
    X = vec.fit_transform(corpus)
    qv = vec.transform([" ".join(tokenize_and_clean(query))])
    scores = (X * qv.T).toarray().ravel()
    idxs = scores.argsort()[::-1]  # desc
    ranked = [(int(i), float(scores[i])) for i in idxs[:topk] if scores[i] > 0]
    return ranked

def find_related_products(question: str, max_rows: int = MAX_PRODUCTS_TO_SHOW) -> pd.DataFrame:
    """‡∏Ñ‡∏∑‡∏ô DataFrame ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏≠‡∏≤‡∏à‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏î‡πâ)"""
    df = _PRODUCTS_DF
    if df.empty:
        return df

    # 1) ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ID ‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÉ‡∏´‡πâ‡πÅ‡∏°‡∏ï‡∏ä‡πå‡∏ï‡∏£‡∏á‡∏Å‡πà‡∏≠‡∏ô
    ids = _extract_possible_ids(question.lower())
    exact_hit = pd.DataFrame(columns=df.columns)
    if ids:
        patt = "|".join(map(re.escape, ids))
        exact_hit = df[df["ID"].str.lower().str.contains(patt, na=False)]
        if not exact_hit.empty:
            return exact_hit.head(max_rows).reset_index(drop=True)

    # 2) ‡πÑ‡∏°‡πà‡∏û‡∏ö ID ‚Üí ‡πÉ‡∏ä‡πâ TF-IDF ‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
    ranked = _tfidf_rank_products(question, df, topk=max_rows)
    if not ranked:
        return pd.DataFrame(columns=df.columns)
    take_idxs = [i for i, _ in ranked]
    out = df.iloc[take_idxs].copy()
    return out.reset_index(drop=True)

def _mk_link_cell(url: str) -> str:
    u = str(url or "").strip()
    if u.lower().startswith("http://") or u.lower().startswith("https://"):
        return f"[link]({u})"
    return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"

def _products_to_markdown_table(pdf: pd.DataFrame) -> str:
    if pdf.empty:
        return ""
    header = "| ID | ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ | ‡∏®‡∏π‡∏ô‡∏¢‡πå | link |\n|---|---|---|---|"
    rows = []
    for _, r in pdf.iterrows():
        rows.append(
            f"| {r['ID']} | {r['‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤']} | {r['‡∏®‡∏π‡∏ô‡∏¢‡πå']} | {_mk_link_cell(r['link'])} |"
        )
    return header + "\n" + "\n".join(rows)

# =========================================================================================

# Function to answer the question based on the context (‡πÄ‡∏î‡∏¥‡∏° + ‡πÅ‡∏ô‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤)
def answer_question(question: str):
    # 1) ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°: ‡∏´‡∏≤ context ‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö TISTR ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ LLaMA ‡∏ï‡∏≠‡∏ö
    context = find_best_context(question)
    if not context:
        base_answer = "‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        product_df = find_related_products(question)
        product_block = _products_to_markdown_table(product_df)
        if product_block:
            return f"{base_answer}\n\n---\n\n**‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤/‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏∏‡∏ì:**\n\n{product_block}"
        return base_answer

    html_content = context.get("HTML", "")[:10000]
    prompt = build_prompt(question, html_content, context)
    reply = ask_llama(prompt)
    base_answer = f"{reply.strip()}\n\nüîó ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: {context.get('URL', '‡πÑ‡∏°‡πà‡∏û‡∏ö URL')}"

    # 2) NEW: ‡πÅ‡∏ô‡∏ö ‚Äú‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‚Äù ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏ñ‡πâ‡∏≤‡∏î‡∏π‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    product_df = find_related_products(question)
    product_block = _products_to_markdown_table(product_df)
    if product_block:
        base_answer += f"\n\n---\n\n**‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤/‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:**\n\n{product_block}"

    return base_answer
